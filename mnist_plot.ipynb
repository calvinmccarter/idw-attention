{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiGQLcogoPlJ"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import resnet18\n",
    "import time\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "METHODS = (\"fc-relu\", \"dot\", \"cosine\", \"neg-dist\", \"exp-neg-dist\", \"inv-dist\", \"neglog-dist\")\n",
    "\n",
    "class SimpleTwoLayerNet(nn.Module):\n",
    "    def __init__(self, method, input_shape, n_classes, n_protos, eps=1e-3, power=10):\n",
    "        super(SimpleTwoLayerNet, self).__init__()\n",
    "        assert method in METHODS\n",
    "        self.method = method\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.n_protos = n_protos\n",
    "        self.eps = eps\n",
    "        self.power = power\n",
    "        self.keys = nn.Parameter(torch.zeros([n_protos] + list(input_shape)))\n",
    "        self.values = nn.Parameter(torch.zeros(n_protos, n_classes))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = torch.flatten(inputs, start_dim=1)\n",
    "        keys = torch.flatten(self.keys, start_dim=1)\n",
    "        values = self.values\n",
    "\n",
    "        if self.method == \"neglog-dist\":\n",
    "            dist = self.eps + torch.cdist(x, keys).pow(self.power)\n",
    "            attn = 1 / dist\n",
    "            attn = attn / attn.sum(dim=1, keepdim=True)\n",
    "            logits = attn @ values\n",
    "        elif self.method == \"inv-dist\":\n",
    "            dist = self.eps + torch.cdist(x, keys).pow(self.power)\n",
    "            attn = 1 / dist\n",
    "            logits = attn.softmax(axis=1) @ values\n",
    "        elif self.method == \"exp-neg-dist\":\n",
    "            dist = torch.cdist(x, keys).pow(self.power)\n",
    "            attn = torch.exp(-0.5 * dist)\n",
    "            logits = attn.softmax(axis=1) @ values\n",
    "        elif self.method == \"neg-dist\":\n",
    "            dist = torch.cdist(x, keys).pow(self.power)\n",
    "            attn = -1 * dist\n",
    "            logits = attn.softmax(axis=1) @ values\n",
    "        elif self.method == \"dot\":\n",
    "            scaling = np.sqrt(1 / self.n_protos)\n",
    "            attn = scaling * x @ keys.T\n",
    "            logits = attn.softmax(axis=1) @ values\n",
    "        elif self.method == \"fc-relu\":\n",
    "            logits = (x @ keys.T).relu() @ values\n",
    "        return logits\n",
    "\n",
    "    def augment(self, inputs, c, idx=None, verbose=False):\n",
    "        x = torch.flatten(inputs, start_dim=1)\n",
    "        keys = torch.flatten(self.keys, start_dim=1)\n",
    "        values = self.values\n",
    "        assert x.shape[0] == 1\n",
    "        assert c < self.n_classes\n",
    "        assert self.method == \"neglog-dist\"\n",
    "\n",
    "        dist = self.eps + torch.cdist(x, keys).pow(self.power)\n",
    "        attn = 1 / dist\n",
    "        attn_norm = attn.sum(dim=1, keepdim=True)\n",
    "        attn = attn / attn_norm\n",
    "        logits = attn @ values\n",
    "        eta = (1 + self.eps * attn_norm.sum()) * (logits.max() - logits[0,c])\n",
    "        newval = eta * F.one_hot(torch.tensor([c]), self.n_classes)\n",
    "        if eta > 0:\n",
    "            if idx is None:\n",
    "                if verbose:\n",
    "                    print(f\"Fixing with new key-value:{eta.item():.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    self.keys = torch.nn.Parameter(torch.concat([self.keys, x], axis=0))\n",
    "                    self.values = torch.nn.Parameter(torch.concat([self.values, newval], axis=0))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Replacing {idx} key-value:{eta.item():.4f}\")\n",
    "                with torch.no_grad():\n",
    "                    self.keys.data[idx, :] = x.detach().clone()\n",
    "                    self.values.data[idx, :] = newval\n",
    "\n",
    "\n",
    "def create_nnnet(X, y, method, n_protos, eps, power):\n",
    "    input_shape = X[0, :].shape\n",
    "    #len(train_df.label.unique())\n",
    "    n_classes = int(1 + torch.max(y))\n",
    "\n",
    "    model = SimpleTwoLayerNet(\n",
    "        method=method, input_shape=input_shape, n_classes=n_classes,\n",
    "        n_protos=n_protos, eps=eps, power=power)\n",
    "\n",
    "    mean = torch.mean(X, axis=0)\n",
    "    std = torch.std(X, axis=0)\n",
    "    mins = torch.min(X, axis=0)[0]\n",
    "    maxs = torch.max(X, axis=0)[0]\n",
    "    protos_size = [n_protos] + list(mean.shape)\n",
    "    keys_np = np.random.normal(\n",
    "        mean, 0.1*std, size=protos_size).astype(np.float32)\n",
    "    #keys_np = np.random.uniform(\n",
    "    #    mins, maxs, size=protos_size).astype(np.float32)\n",
    "    #    nn.init.zeros_(self.values)\n",
    "    values_np = np.zeros((n_protos, n_classes)).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        if method == \"fc-relu\":\n",
    "            nn.init.kaiming_uniform_(model.keys, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_uniform_(model.values)\n",
    "        elif method == \"dot\":\n",
    "            nn.init.xavier_uniform_(model.keys)\n",
    "            nn.init.xavier_uniform_(model.values)\n",
    "            nn.init.zeros_(model.keys)\n",
    "            nn.init.kaiming_uniform_(model.keys)\n",
    "            nn.init.kaiming_uniform_(model.values)\n",
    "        else:\n",
    "            model.keys.data = torch.tensor(keys_np)\n",
    "            model.values.data = torch.tensor(values_np)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwVEN4e8ppxZ"
   },
   "outputs": [],
   "source": [
    "seed=12345\n",
    "batch_size=4\n",
    "test_batch_size=1000\n",
    "lr=0.001\n",
    "epochs=50\n",
    "dry_run=False\n",
    "log_interval=3000\n",
    "save_model=False\n",
    "use_cuda = False\n",
    "use_mps = False\n",
    "method = \"neglog-dist\"\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "dataset1 = datasets.MNIST(\n",
    "    '../data', train=True, download=True, transform=train_transform)\n",
    "dataset2 = datasets.MNIST(\n",
    "    '../data', train=False, transform=test_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "train_X = torch.concat([datum[0] for datum in train_loader], axis=0)\n",
    "train_y = torch.concat([datum[1] for datum in train_loader], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIOLrzsFrQI-"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{:04d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss\n",
    "\n",
    "def print_digit_key_counts(model):\n",
    "    cnt = Counter({_: 0 for _ in range(10)})\n",
    "    for i in range(0, model.keys.shape[0]):\n",
    "        amax = torch.argmax(model.values[i,:]).item();\n",
    "        cnt[amax] += 1\n",
    "    print(cnt)\n",
    "\n",
    "\n",
    "model = create_nnnet(X=train_X, y=train_y, n_protos=20, method=method, eps=1e-3, power=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, amsgrad=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "cos_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "    print_digit_key_counts(model)\n",
    "    test_loss = test(model, device, test_loader)\n",
    "    cos_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rs_hLuIsswYP"
   },
   "outputs": [],
   "source": [
    "\n",
    "#torch.save(model.state_dict(), '/content/drive/My Drive/inverse-distance-mnist-plot-20protos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBG2JULpciqn"
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "key_to_digit = []\n",
    "for i in range(0, model.keys.shape[0]):\n",
    "    amax = torch.argmax(model.values[i,:]).item();\n",
    "    key_to_digit.append((i, amax))\n",
    "    cnt[amax] += 1\n",
    "print(cnt)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(7,7/5.), dpi=150);\n",
    "sorted_keys = sorted(key_to_digit, key=lambda kd: kd[1])\n",
    "for plotix, (k, d) in enumerate(sorted_keys):\n",
    "    axes[plotix // 10, plotix % 10].imshow(model.keys[k,0, :, :].detach().numpy())\n",
    "    #axes[plotix // 10, plotix % 10].set_title(f\"{d}\")\n",
    "    axes[plotix // 10, plotix % 10].set_xticks([])\n",
    "    axes[plotix // 10, plotix % 10].set_yticks([])\n",
    "#plt.tight_layout();\n",
    "plt.savefig(f'/content/drive/My Drive/inverse-distance-mnist-{method}-plot-20protos.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfH0wXxUgNHt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
